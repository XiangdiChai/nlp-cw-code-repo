{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "approach2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XiangdiChai/nlp-cw-code-repo/blob/master/approach2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nw0djIvK0Z-"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from torch.utils.data import Dataset, random_split\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from gensim.models import Word2Vec\r\n",
        "import codecs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmxnmeAGKwas"
      },
      "source": [
        "# Load data\r\n",
        "#feel free to edit the path\r\n",
        "train_df = pd.read_csv('train.csv')\r\n",
        "dev_df = pd.read_csv('dev.csv')\r\n",
        "test_df = pd.read_csv('test.csv')\r\n",
        "news_df = pd.read_csv('abcnews-date-text.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3OX3I1XK-vr"
      },
      "source": [
        "# Number of epochs\r\n",
        "epochs = 10\r\n",
        "\r\n",
        "use_cuda = torch.cuda.is_available()\r\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0txPa04K_It"
      },
      "source": [
        "# We define our training loop\r\n",
        "def train(train_iter, dev_iter, model, number_epoch):\r\n",
        "    \"\"\"\r\n",
        "    Training loop for the model, which calls on eval to evaluate after each epoch\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    \r\n",
        "    print(\"Training model.\")\r\n",
        "    best_loss = 10\r\n",
        "    for epoch in range(1, number_epoch+1):\r\n",
        "\r\n",
        "        model.train()\r\n",
        "        epoch_loss = 0\r\n",
        "        epoch_sse = 0\r\n",
        "        no_observations = 0  # Observations used for training so far\r\n",
        "        \r\n",
        "        for batch in train_iter:\r\n",
        "            \r\n",
        "            feature, target = batch\r\n",
        "\r\n",
        "            feature, target = feature.to(device), target.to(device)\r\n",
        "            \r\n",
        "            # for RNN:\r\n",
        "            model.batch_size = target.shape[0]\r\n",
        "            no_observations = no_observations + target.shape[0]\r\n",
        "            model.hidden = model.init_hidden()\r\n",
        "            predictions = model(feature).squeeze(1)\r\n",
        "            \r\n",
        "            optimizer.zero_grad()\r\n",
        "\r\n",
        "            loss = loss_fn(predictions, target)\r\n",
        "\r\n",
        "            sse, __ = model_performance(predictions.detach().cpu().numpy(), target.detach().cpu().numpy())\r\n",
        "\r\n",
        "            loss.backward()\r\n",
        "            optimizer.step()\r\n",
        "\r\n",
        "            epoch_loss += loss.item()*target.shape[0]\r\n",
        "            epoch_sse += sse\r\n",
        "\r\n",
        "        valid_loss, valid_mse, __, __ = eval(dev_iter, model)\r\n",
        "        \r\n",
        "        if valid_loss < best_loss:\r\n",
        "          best_loss = valid_loss \r\n",
        "          \r\n",
        "          torch.save(model.state_dict(),  './sample_data/BiLSTM_model_1.pth')\r\n",
        "          \r\n",
        "          print('model save ', best_loss)\r\n",
        "\r\n",
        "        epoch_loss, epoch_mse = epoch_loss / no_observations, epoch_sse / no_observations\r\n",
        "        print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.2f} | Train MSE: {epoch_mse:.2f} | Train RMSE: {epoch_mse**0.5:.2f} | \\\r\n",
        "        Val. Loss: {valid_loss:.2f} | Val. MSE: {valid_mse:.2f} |  Val. RMSE: {valid_mse**0.5:.4f} |')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ScimhCMLBZO"
      },
      "source": [
        "# We evaluate performance on our dev set\r\n",
        "def eval(data_iter, model):\r\n",
        "    \"\"\"\r\n",
        "    Evaluating model performance on the dev set\r\n",
        "    \"\"\"\r\n",
        "    model.eval()\r\n",
        "    epoch_loss = 0\r\n",
        "    epoch_sse = 0\r\n",
        "    pred_all = []\r\n",
        "    trg_all = []\r\n",
        "    no_observations = 0\r\n",
        "\r\n",
        "    with torch.no_grad():\r\n",
        "        for batch in data_iter:\r\n",
        "            feature, target = batch\r\n",
        "\r\n",
        "            feature, target = feature.to(device), target.to(device)\r\n",
        "\r\n",
        "            # for RNN:\r\n",
        "            model.batch_size = target.shape[0]\r\n",
        "            no_observations = no_observations + target.shape[0]\r\n",
        "            model.hidden = model.init_hidden()\r\n",
        "\r\n",
        "            predictions = model(feature).squeeze(1)\r\n",
        "            loss = loss_fn(predictions, target)\r\n",
        "\r\n",
        "            # We get the mse\r\n",
        "            pred, trg = predictions.detach().cpu().numpy(), target.detach().cpu().numpy()\r\n",
        "            sse, __ = model_performance(pred, trg)\r\n",
        "\r\n",
        "            epoch_loss += loss.item()*target.shape[0]\r\n",
        "            epoch_sse += sse\r\n",
        "            pred_all.extend(pred)\r\n",
        "            trg_all.extend(trg)\r\n",
        "\r\n",
        "    return epoch_loss/no_observations, epoch_sse/no_observations, np.array(pred_all), np.array(trg_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipPcr7R8LDAD"
      },
      "source": [
        "# How we print the model performance\r\n",
        "def model_performance(output, target, print_output=False):\r\n",
        "    \"\"\"\r\n",
        "    Returns SSE and MSE per batch (printing the MSE and the RMSE)\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    sq_error = (output - target)**2\r\n",
        "\r\n",
        "    sse = np.sum(sq_error)\r\n",
        "    mse = np.mean(sq_error)\r\n",
        "    rmse = np.sqrt(mse)\r\n",
        "\r\n",
        "    if print_output:\r\n",
        "        print(f'| MSE: {mse:.2f} | RMSE: {rmse:.2f} |')\r\n",
        "\r\n",
        "    return sse, mse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9nHk_VuLFR8"
      },
      "source": [
        "import string\r\n",
        "import re\r\n",
        "\r\n",
        "\r\n",
        "def create_vocab(data):\r\n",
        "    \"\"\"\r\n",
        "    Creating a corpus of all the tokens used\r\n",
        "    \"\"\"\r\n",
        "    tokenized_corpus = [] # Let us put the tokenized corpus in a list\r\n",
        "\r\n",
        "    vocabulary = []\r\n",
        "\r\n",
        "    for sentence in data:\r\n",
        "        \r\n",
        "        sentence = sentence.lower() #lower case\r\n",
        "        sentence = sentence.translate(str.maketrans('', '', string.punctuation)) #remove punctuation\r\n",
        "        sentence = re.sub(r'\\d+', '', sentence) #remove number\r\n",
        "        \r\n",
        "        tokenized_sentence = []\r\n",
        "        \r\n",
        "        for token in sentence.split(' '): \r\n",
        "\r\n",
        "            tokenized_sentence.append(token)\r\n",
        "\r\n",
        "            if token not in vocabulary:\r\n",
        "                \r\n",
        "                vocabulary.append(token)\r\n",
        "\r\n",
        "\r\n",
        "        tokenized_corpus.append(tokenized_sentence)\r\n",
        "\r\n",
        "    return vocabulary, tokenized_corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVgpp9JNLIBO"
      },
      "source": [
        "def collate_fn_padd(batch):\r\n",
        "    '''\r\n",
        "    We add padding to our minibatches and create tensors for our model\r\n",
        "    '''\r\n",
        "\r\n",
        "    batch_labels = [l for f, l in batch]\r\n",
        "    batch_features = [f for f, l in batch]\r\n",
        "\r\n",
        "    batch_features_len = [len(f) for f, l in batch]\r\n",
        "\r\n",
        "    seq_tensor = torch.zeros((len(batch), max(batch_features_len))).long()\r\n",
        "\r\n",
        "    for idx, (seq, seqlen) in enumerate(zip(batch_features, batch_features_len)):\r\n",
        "        seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\r\n",
        "\r\n",
        "    batch_labels = torch.FloatTensor(batch_labels)\r\n",
        "\r\n",
        "    return seq_tensor, batch_labels\r\n",
        "\r\n",
        "class Task1Dataset(Dataset):\r\n",
        "\r\n",
        "    def __init__(self, train_data, labels):\r\n",
        "        self.x_train = train_data\r\n",
        "        self.y_train = labels\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.y_train)\r\n",
        "\r\n",
        "    def __getitem__(self, item):\r\n",
        "        return self.x_train[item], self.y_train[item]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj8Zbh80LJcf"
      },
      "source": [
        "class BiLSTM(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, batch_size, device):\r\n",
        "        super(BiLSTM, self).__init__()\r\n",
        "        self.hidden_dim = hidden_dim\r\n",
        "        self.embedding_dim = embedding_dim\r\n",
        "        self.device = device\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\r\n",
        "\r\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\r\n",
        "        # with dimensionality hidden_dim.\r\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\r\n",
        "\r\n",
        "        # The linear layer that maps from hidden state space to tag space\r\n",
        "        \r\n",
        "        self.hidden2label = nn.Sequential(nn.Linear(hidden_dim * 2, hidden_dim),  #nn.Linear(hidden_dim * 2, 1)\r\n",
        "                          nn.LeakyReLU(),\r\n",
        "                          nn.Linear(hidden_dim, hidden_dim//2),\r\n",
        "                          nn.LeakyReLU(),\r\n",
        "                          nn.Linear(hidden_dim//2, 1),\r\n",
        "                          nn.LeakyReLU())\r\n",
        "       \r\n",
        "        self.hidden = self.init_hidden()\r\n",
        "\r\n",
        "    def init_hidden(self):\r\n",
        "        # Before we've done anything, we dont have any hidden state.\r\n",
        "        # Refer to the Pytorch documentation to see exactly why they have this dimensionality.\r\n",
        "        # The axes semantics are (num_layers * num_directions, minibatch_size, hidden_dim)\r\n",
        "        return torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device), \\\r\n",
        "               torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device)\r\n",
        "\r\n",
        "    def forward(self, sentence):\r\n",
        "       \r\n",
        "        embedded = self.embedding(sentence)\r\n",
        "        embedded = embedded.permute(1, 0, 2)\r\n",
        "\r\n",
        "        lstm_out, self.hidden = self.lstm(\r\n",
        "            embedded.view(len(embedded), self.batch_size, self.embedding_dim), self.hidden)\r\n",
        "        out = self.hidden2label(lstm_out[-1])\r\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uth4LlqiLK6C"
      },
      "source": [
        "\r\n",
        "def pre_processing(method):\r\n",
        "\r\n",
        "  training_origin_data = train_df['original']\r\n",
        "  training_edit_data = train_df['edit']\r\n",
        "  dev_origin_data = dev_df['original']\r\n",
        "  dev_edit_data = dev_df['edit']\r\n",
        "  test_origin_data = test_df['original']\r\n",
        "  test_edit_data = test_df['edit']\r\n",
        "\r\n",
        "  training_data = []\r\n",
        "  dev_data = []\r\n",
        "  test_data = []\r\n",
        "\r\n",
        "\r\n",
        "  #Replace the word in the bracket with the edit word \r\n",
        "  if method == 1: \r\n",
        "\r\n",
        "    for i in range(len(training_origin_data)): \r\n",
        "      training_data.append(training_origin_data[i].replace(training_origin_data[i][training_origin_data[i].find(\"<\") : training_origin_data[i].find(\">\")+1],training_edit_data[i]))\r\n",
        "    for i in range(len(dev_origin_data)): \r\n",
        "      dev_data.append(dev_origin_data[i].replace(dev_origin_data[i][dev_origin_data[i].find(\"<\") : dev_origin_data[i].find(\">\")+1],dev_edit_data[i]))\r\n",
        "    for i in range(len(test_origin_data)): \r\n",
        "      test_data.append(test_origin_data[i].replace(test_origin_data[i][test_origin_data[i].find(\"<\") : test_origin_data[i].find(\">\")+1],test_edit_data[i]))\r\n",
        "\r\n",
        "  #Show both original sentence and the edit sentence to the network\r\n",
        "  elif method == 2:\r\n",
        "    for i in range(len(training_origin_data)): \r\n",
        "      training_data.append(training_origin_data[i]+' '+training_origin_data[i].replace(training_origin_data[i][training_origin_data[i].find(\"<\") : training_origin_data[i].find(\">\")+1],training_edit_data[i]))\r\n",
        "    for i in range(len(dev_origin_data)): \r\n",
        "      dev_data.append(dev_origin_data[i]+' '+dev_origin_data[i].replace(dev_origin_data[i][dev_origin_data[i].find(\"<\") : dev_origin_data[i].find(\">\")+1],dev_edit_data[i]))\r\n",
        "    for i in range(len(test_origin_data)): \r\n",
        "      test_data.append(test_origin_data[i]+' '+test_origin_data[i].replace(test_origin_data[i][test_origin_data[i].find(\"<\") : test_origin_data[i].find(\">\")+1],test_edit_data[i]))\r\n",
        "  \r\n",
        "  #Append the edit word at the end of the original sentence.\r\n",
        "  elif method == 3:\r\n",
        "\r\n",
        "    for i in range(len(training_origin_data)): \r\n",
        "      training_data.append(training_origin_data[i]+' '+training_edit_data[i])\r\n",
        "    for i in range(len(dev_origin_data)): \r\n",
        "      dev_data.append(dev_origin_data[i]+' '+dev_edit_data[i])  \r\n",
        "    for i in range(len(test_origin_data)):\r\n",
        "      test_data.append(test_origin_data[i]+' '+test_edit_data[i])\r\n",
        "\r\n",
        "  #Append the edit word after the word needs to be replaced. \r\n",
        "  elif method == 4:\r\n",
        "    for i in range(len(training_origin_data)): \r\n",
        "      word = training_origin_data[i][training_origin_data[i].find(\"<\") : training_origin_data[i].find(\">\")]\r\n",
        "      training_data.append(training_origin_data[i].replace(word, word+' '+training_edit_data[i]))\r\n",
        "\r\n",
        "    for i in range(len(dev_origin_data)): \r\n",
        "      word = dev_origin_data[i][dev_origin_data[i].find(\"<\") : dev_origin_data[i].find(\">\")]\r\n",
        "      dev_data.append(dev_origin_data[i].replace(word, word+' '+dev_edit_data[i]))\r\n",
        "\r\n",
        "    for i in range(len(test_origin_data)): \r\n",
        "      word = test_origin_data[i][test_origin_data[i].find(\"<\") : test_origin_data[i].find(\">\")]\r\n",
        "      test_data.append(test_origin_data[i].replace(word, word+' '+test_edit_data[i]))\r\n",
        "\r\n",
        "  return training_data,dev_data,test_data\r\n",
        "\r\n",
        "\r\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVbf9MzoLODN"
      },
      "source": [
        "def word_corpus(method):\r\n",
        "    training_origin_data = train_df['original']\r\n",
        "    training_edit_data = train_df['edit']\r\n",
        " \r\n",
        "    dev_origin_data = dev_df['original']\r\n",
        "    dev_edit = dev_df['edit']\r\n",
        "\r\n",
        "    test_origin_data = test_df['original']\r\n",
        "    test_edit_data = test_df['edit']\r\n",
        "\r\n",
        "    \r\n",
        "\r\n",
        "    add_data = []\r\n",
        "\r\n",
        "    #use training, validation and test original sentence\r\n",
        "    for i in range(len(training_origin_data)): \r\n",
        "        add_data.append(training_origin_data[i])\r\n",
        "    for i in range(len(dev_origin_data)): \r\n",
        "        add_data.append(dev_origin_data[i])\r\n",
        "    for i in range(len(test_origin_data)):\r\n",
        "        add_data.append(test_origin_data[i])   \r\n",
        "    if method == 1:\r\n",
        "        return add_data\r\n",
        "\r\n",
        "    #add additional news headlines\r\n",
        "    news_data = news_df['headline_text']\r\n",
        "    for i in range(len(news_data)):\r\n",
        "        add_data.append(news_data[i])  \r\n",
        "    if method == 2:\r\n",
        "      return add_data\r\n",
        "        \r\n",
        "    \r\n",
        "    #add edit sentence into the dataset        \r\n",
        "    for i in range(len(training_origin_data)): \r\n",
        "        add_data.append(training_origin_data[i].replace(training_origin_data[i][training_origin_data[i].find(\"<\") : training_origin_data[i].find(\">\")+1],training_edit_data[i]))\r\n",
        "    for i in range(len(dev_origin_data)): \r\n",
        "        add_data.append(dev_origin_data[i].replace(dev_origin_data[i][dev_origin_data[i].find(\"<\") : dev_origin_data[i].find(\">\")+1],dev_edit_data[i]))\r\n",
        "    for i in range(len(test_origin_data)): \r\n",
        "        add_data.append(test_origin_data[i].replace(test_origin_data[i][test_origin_data[i].find(\"<\") : test_origin_data[i].find(\">\")+1],test_edit_data[i]))\r\n",
        "    if method == 3:\r\n",
        "      return add_data\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AcKhJI6LRPZ"
      },
      "source": [
        "#change number in word_corpus(number) to experience with different corpus\r\n",
        "#if you change to number 2 and 3, it will take a while to trian the word embedding\r\n",
        "add_data = word_corpus(2)\r\n",
        "news_vocab, news_tokenized_corpus = create_vocab(add_data)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQT6Bz964BDK"
      },
      "source": [
        "#train own word to vector model\r\n",
        "own_embedding_model = Word2Vec(news_tokenized_corpus, min_count=0, size = 200, window = 3, iter = 30)\r\n",
        "own_embedding_model.wv.save_word2vec_format(\"./own_model.txt\")\r\n",
        "own_embedding_model.save(\"./word2vec.model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkydFxxOLYuw",
        "outputId": "f43963ab-271b-467b-b265-8c3ab1ec7e92"
      },
      "source": [
        "print(own_embedding_model.most_similar('trump'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[('trumps', 0.6814748048782349), ('obama', 0.6776908040046692), ('turmp', 0.6068863868713379), ('putin', 0.5586628913879395), ('russia', 0.5517419576644897), ('obamas', 0.5415406227111816), ('assad', 0.5403751730918884), ('clinton', 0.5335540175437927), ('hillary', 0.5273513793945312), ('donald', 0.5260993242263794)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEqkvAEnLcQ_",
        "outputId": "d22d02c2-e191-44f9-ff8b-ba1c7586bc3d"
      },
      "source": [
        "\r\n",
        "training_data,dev_data,test_data = pre_processing(3)\r\n",
        "\r\n",
        "# Creating word vectors\r\n",
        "training_vocab, training_tokenized_corpus = create_vocab(training_data)\r\n",
        "dev_vocab, dev_tokenized_corpus = create_vocab(dev_data)\r\n",
        "test_vocab, test_tokenized_corpus = create_vocab(test_data)\r\n",
        "\r\n",
        "\r\n",
        "# Creating joint vocab from test and train:\r\n",
        "joint_vocab, joint_tokenized_corpus = create_vocab(training_data+test_data+dev_data)\r\n",
        "\r\n",
        "print(\"Vocab created.\")\r\n",
        "\r\n",
        "\r\n",
        "idx_2_word = {}\r\n",
        "word_2_idx = {}\r\n",
        "word_vec = []\r\n",
        "\r\n",
        "#Change if you have not run the training w2v code above \r\n",
        "#with codecs.open('own_model_method2_200d.txt', 'r','utf-8') as f:\r\n",
        "#with codecs.open('own_model_method3_200d.txt', 'r','utf-8') as f:\r\n",
        "with codecs.open('own_model.txt', 'r','utf-8') as f:\r\n",
        "  index = 0\r\n",
        "  for line in f.readlines():\r\n",
        "    # Ignore the first line - first line typically contains vocab, dimensionality\r\n",
        "    if len(line.strip().split()) > 3:\r\n",
        "      word = line.strip().split()[0]\r\n",
        "      if word in joint_vocab:        \r\n",
        "        (word, vec) = (word,list(map(float,line.strip().split()[1:])))\r\n",
        "        word_vec.append(vec)#own_embedding_model[word])\r\n",
        "        idx_2_word[index] = word\r\n",
        "        word_2_idx[word] = index\r\n",
        "        index += 1\r\n",
        "\r\n",
        "vectorized_seqs = [[word_2_idx[tok] for tok in seq if tok in word_2_idx] for seq in training_tokenized_corpus]\r\n",
        "\r\n",
        "vectorized_seqs = [x if len(x) > 0 else [0] for x in vectorized_seqs]\r\n",
        "\r\n",
        "\r\n",
        "dev_vectorized_seqs = [[word_2_idx[tok] for tok in seq if tok in word_2_idx] for seq in dev_tokenized_corpus]\r\n",
        "\r\n",
        "dev_vectorized_seqs = [x if len(x) > 0 else [0] for x in dev_vectorized_seqs]\r\n",
        "\r\n",
        "\r\n",
        "test_vectorized_seqs = [[word_2_idx[tok] for tok in seq if tok in word_2_idx] for seq in  test_tokenized_corpus]\r\n",
        "test_vectorized_seqs = [x if len(x) > 0 else [0] for x in test_vectorized_seqs]\r\n",
        "\r\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fc_v50u2Ld1D",
        "outputId": "63f05a0e-7c14-43a2-cb3d-e326547758bd"
      },
      "source": [
        "word_vec = np.asarray(word_vec)\r\n",
        "INPUT_DIM = len(word_2_idx)\r\n",
        "EMBEDDING_DIM = 200\r\n",
        "BATCH_SIZE = 32\r\n",
        "\r\n",
        "model = BiLSTM(EMBEDDING_DIM, 256, INPUT_DIM, BATCH_SIZE, device)\r\n",
        "print(\"Model initialised.\")\r\n",
        "\r\n",
        "model.to(device)\r\n",
        "# We provide the model with our embeddings\r\n",
        "model.embedding.weight.data.copy_(torch.from_numpy(word_vec))\r\n",
        "\r\n",
        "\r\n",
        "feature = vectorized_seqs\r\n",
        "dev_feature = dev_vectorized_seqs\r\n",
        "\r\n",
        "# 'feature' is a list of lists, each containing embedding IDs for word tokens\r\n",
        "training = Task1Dataset(feature, train_df['meanGrade'])\r\n",
        "dev = Task1Dataset(dev_feature, dev_df['meanGrade'])\r\n",
        "\r\n",
        "train_loader = torch.utils.data.DataLoader(training, shuffle=True, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\r\n",
        "dev_loader = torch.utils.data.DataLoader(dev, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\r\n",
        "\r\n",
        "print(\"Dataloaders created.\")\r\n",
        "\r\n",
        "loss_fn = nn.MSELoss()\r\n",
        "loss_fn = loss_fn.to(device)\r\n",
        "\r\n",
        "optimizer = torch.optim.Adam(model.parameters())\r\n",
        "\r\n",
        "train(train_loader, dev_loader, model, epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model initialised.\n",
            "Dataloaders created.\n",
            "Training model.\n",
            "model save  0.35392256638588815\n",
            "| Epoch: 01 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |         Val. Loss: 0.35 | Val. MSE: 0.35 |  Val. RMSE: 0.5949 |\n",
            "model save  0.3344501690240673\n",
            "| Epoch: 02 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |         Val. Loss: 0.33 | Val. MSE: 0.33 |  Val. RMSE: 0.5783 |\n",
            "model save  0.31975420928780035\n",
            "| Epoch: 03 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |         Val. Loss: 0.32 | Val. MSE: 0.32 |  Val. RMSE: 0.5655 |\n",
            "| Epoch: 04 | Train Loss: 0.25 | Train MSE: 0.25 | Train RMSE: 0.50 |         Val. Loss: 0.34 | Val. MSE: 0.34 |  Val. RMSE: 0.5856 |\n",
            "| Epoch: 05 | Train Loss: 0.18 | Train MSE: 0.18 | Train RMSE: 0.42 |         Val. Loss: 0.36 | Val. MSE: 0.36 |  Val. RMSE: 0.6015 |\n",
            "| Epoch: 06 | Train Loss: 0.13 | Train MSE: 0.13 | Train RMSE: 0.36 |         Val. Loss: 0.39 | Val. MSE: 0.39 |  Val. RMSE: 0.6255 |\n",
            "| Epoch: 07 | Train Loss: 0.10 | Train MSE: 0.10 | Train RMSE: 0.32 |         Val. Loss: 0.40 | Val. MSE: 0.40 |  Val. RMSE: 0.6303 |\n",
            "| Epoch: 08 | Train Loss: 0.08 | Train MSE: 0.08 | Train RMSE: 0.28 |         Val. Loss: 0.43 | Val. MSE: 0.43 |  Val. RMSE: 0.6531 |\n",
            "| Epoch: 09 | Train Loss: 0.06 | Train MSE: 0.06 | Train RMSE: 0.24 |         Val. Loss: 0.43 | Val. MSE: 0.43 |  Val. RMSE: 0.6551 |\n",
            "| Epoch: 10 | Train Loss: 0.04 | Train MSE: 0.04 | Train RMSE: 0.20 |         Val. Loss: 0.44 | Val. MSE: 0.44 |  Val. RMSE: 0.6668 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGaaBg6OLef_"
      },
      "source": [
        "\r\n",
        "test_feature = test_vectorized_seqs\r\n",
        "\r\n",
        "test = Task1Dataset(test_feature, train_df['meanGrade'][0:len(test_feature)])\r\n",
        "test_loader = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\r\n",
        "\r\n",
        "predictions = []\r\n",
        "best_model = BiLSTM(EMBEDDING_DIM, 256, INPUT_DIM, BATCH_SIZE, device)\r\n",
        "best_model.load_state_dict(torch.load('/content/sample_data/BiLSTM_model_1.pth'))\r\n",
        "\r\n",
        "best_model.eval()\r\n",
        "with torch.no_grad():\r\n",
        "  for batch in test_loader:\r\n",
        "    feature, target = batch\r\n",
        "    feature, target = feature.to(device), target.to(device)\r\n",
        "    feature = torch.LongTensor(feature)\r\n",
        "    best_model.batch_size = target.shape[0]\r\n",
        "    best_model.hidden = best_model.init_hidden()\r\n",
        "    prediction = best_model(feature).squeeze(1)\r\n",
        "    pred = prediction.detach().cpu().numpy()  \r\n",
        "    pred = np.reshape(pred,(len(pred),1))\r\n",
        "    for p in pred:\r\n",
        "      predictions.append(p)\r\n",
        "\r\n",
        "predictions = np.asarray(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6uGAGBmLgik"
      },
      "source": [
        "import csv\r\n",
        "out_loc = '/content/sample_data/task-1-output.csv'\r\n",
        "with open(out_loc, \"w\") as f:\r\n",
        "    writer = csv.writer(f)\r\n",
        "    writer.writerow(('id','pred'))\r\n",
        "    for i in range(len(test_df['id'])):\r\n",
        "        writer.writerow((test_df['id'][i],predictions[i][0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8PbOj-J_g3r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}